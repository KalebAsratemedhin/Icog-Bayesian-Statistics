{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: Bayesian Statistics\n",
    "Bayesian statistics is an approach to statistical inference that applies Bayes' Theorem to update the probability estimate for a hypothesis as more evidence or data becomes available. Unlike frequentist statistics, which only considers the data at hand, Bayesian statistics incorporates prior knowledge or beliefs about the world and updates these beliefs as new data is observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import requests\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kaleb/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset and unzip folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(url, download_path):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(download_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded the file to {download_path}\")\n",
    "    else:\n",
    "        print(\"Failed to download the file\")\n",
    "\n",
    "    with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"../data\")\n",
    "        print(\"Unzipped the dataset to '../data' folder\")\n",
    "\n",
    "    extracted_files = os.listdir(\"../data\")\n",
    "    print(\"Extracted files:\", extracted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded the file to ../data/smsspamcollection.zip\n",
      "Unzipped the dataset to '../data' folder\n",
      "Extracted files: ['smsspamcollection.zip', 'readme', 'SMSSpamCollection']\n"
     ]
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "download_path = \"../data/smsspamcollection.zip\"\n",
    "\n",
    "download_dataset(url, download_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "df = pd.read_csv('../data/SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    5572 non-null   object\n",
      " 1   message  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to binary values: ham = 0, spam = 1\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text (remove punctuation, convert to lowercase, remove stopwords)\n",
    "def clean_text(text):\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['message'] = df['message'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset to training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to numerical features using CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Tokenize the messages and create vocabulary\n",
    "def tokenize(messages):\n",
    "    word_counts = defaultdict(int)\n",
    "    for message in messages:\n",
    "        for word in message.split():\n",
    "            word_counts[word] += 1\n",
    "    return word_counts\n",
    "\n",
    "\n",
    "# Step 5: Calculate Prior Probabilities P(spam) and P(ham)\n",
    "def calculate_priors(y_train):\n",
    "    spam_count = sum(y_train)\n",
    "    ham_count = len(y_train) - spam_count\n",
    "    total_count = len(y_train)\n",
    "    p_spam = spam_count / total_count\n",
    "    p_ham = ham_count / total_count\n",
    "    return p_spam, p_ham\n",
    "\n",
    "# Step 6: Calculate Likelihoods P(word|spam) and P(word|ham)\n",
    "def calculate_likelihoods(X_train, y_train, vocab):\n",
    "    spam_word_counts = defaultdict(int)\n",
    "    ham_word_counts = defaultdict(int)\n",
    "    spam_count = 0\n",
    "    ham_count = 0\n",
    "\n",
    "    for message, label in zip(X_train, y_train):\n",
    "        words = message.split()\n",
    "        if label == 1:  # Spam\n",
    "            spam_count += 1\n",
    "            for word in words:\n",
    "                spam_word_counts[word] += 1\n",
    "        else:  # Ham\n",
    "            ham_count += 1\n",
    "            for word in words:\n",
    "                ham_word_counts[word] += 1\n",
    "\n",
    "    # Apply Laplace smoothing\n",
    "    vocab_size = len(vocab)\n",
    "    p_word_given_spam = {}\n",
    "    p_word_given_ham = {}\n",
    "\n",
    "    for word in vocab:\n",
    "        p_word_given_spam[word] = (spam_word_counts[word] + 1) / (spam_count + vocab_size)\n",
    "        p_word_given_ham[word] = (ham_word_counts[word] + 1) / (ham_count + vocab_size)\n",
    "\n",
    "    return p_word_given_spam, p_word_given_ham\n",
    "\n",
    "# Step 7: Predict the class for a new message\n",
    "def predict(message, p_spam, p_ham, p_word_given_spam, p_word_given_ham, vocab):\n",
    "    p_spam_given_message = math.log(p_spam)\n",
    "    p_ham_given_message = math.log(p_ham)\n",
    "\n",
    "    words = message.split()\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            p_spam_given_message += math.log(p_word_given_spam.get(word, 1 / (len(vocab) + 1)))\n",
    "            p_ham_given_message += math.log(p_word_given_ham.get(word, 1 / (len(vocab) + 1)))\n",
    "\n",
    "    if p_spam_given_message > p_ham_given_message:\n",
    "        return 1  # Spam\n",
    "    else:\n",
    "        return 0  # Ham\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "def evaluate(X_test, y_test, p_spam, p_ham, p_word_given_spam, p_word_given_ham, vocab):\n",
    "    correct = 0\n",
    "    for message, true_label in zip(X_test, y_test):\n",
    "        predicted_label = predict(message, p_spam, p_ham, p_word_given_spam, p_word_given_ham, vocab)\n",
    "        if predicted_label == true_label:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9821\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the training messages and create a vocabulary\n",
    "vocab = tokenize(X_train)\n",
    "\n",
    "# Calculate prior probabilities\n",
    "p_spam, p_ham = calculate_priors(y_train)\n",
    "\n",
    "# Calculate likelihoods for each word in the vocabulary\n",
    "p_word_given_spam, p_word_given_ham = calculate_likelihoods(X_train, y_train, vocab)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluate(X_test, y_test, p_spam, p_ham, p_word_given_spam, p_word_given_ham, vocab)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
